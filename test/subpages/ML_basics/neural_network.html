<!DOCTYPE html>
<html>


<section>
    References.
    <ul><li>
       <a href="../../Computer Science/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning-MIT (2017).pdf">Deep Learning</a>,  MIT. 
    </li></ul>
</section>
<section>
    <h3>What are questions that ML try to address?</h3>

    <p>
        <ul>
            <li>Generalization - the ablity to perform well on new inputs.
                This implies, to even improve performance we need  a <em>test set</em> and <em>train set</em>.
                Now one has a balance
                <ul>
                    <li>training  error small,  i.e. <em>underfittig</em>.</li>
                    <li>Disparity between training and test error small. <em>overfitting</em>. </li>
                </ul>
               See  Fig 5.3.  One measure of possibility of overfitting is that of <em>capacity</em>. 
            </li>
            
        </ul>

    </p>
</section>


    <section>
    <h3>Neural networks (NN)</h3> 

    <p>
        FAQs.  
        <ul>
            <li> Why don't we <i>understand</i> NN? </li>
        </ul>
    </p>
    <p>
    Some practical problems
    <ul>
    <li>
      How many hidden layers/units should I use? 
      Even experienced ML users don't know. The problem seems to rather be 
      increasing the speed of the <i>testing cycle</i>.
    </li>
    <li>
        ... 
    </li>
    <!-- <li>What makes neural network better than the perceptron?</li> -->
    </ul>
    
    </p>
    </section>


    <section>
    <p>The classic text processing. 
        <ul>

        </ul>
    </p>
    </section>

    <section>

        <h5>Is there a general way to pictorialize the provess of ML algorithms?</h5>


    </section>


    <section>
        <h3>Review of Supervised learning</h3>

        <p>
            In terms of probability, given x and y...? <br> 
            The flow is broken down as 
            <ol>
                <li>Input. </li>
                <li> Minimize Error. i.e. MSE. </li>
                <li>Apply gradient descent.</li>
            </ol>

            Ex. in the case of Linear regression, this is the  <em>normal equations</em>.  
        </p>

        <p>Regularization.</p>
    </section>
    <section>
        <h4>Review of feed forward networks</h4>
<p>  Why can one not approximate nonlinaer functions? <br>
Ex: take the XOR  function. </p>
    </section>

<h3>
    RNNs. 
</h3>

<section>
    Applications.
    <table 
    #style="width:100%"
    >
        <tr>
            <th> m:n</th>
            <th> n=1 </th>
            <th>  n>1 </th>
        </tr>
        <tr>
            <th>m=1</th>
            <th>
               Scores of game
            </th>
            <th>
                Caption of image
            </th>
        </tr>
        <tr>
            <th>m>1</th>
            <th>sentiment of tweet</th>
            <th>translation</th>
        </tr>
        <th>

        </th>
    </table> 

    <p>
        The most basic example is Vanilla RNN, but it has a short coming of vanishing/exploding gradients, 
        see <a href="https://www.youtube.com/watch?v=S27pHKBEp30">first 5 minutes</a>. 
    </p>
</section>


    <h3>
        Convolution neural nets (CNN)
    </h3>

    <section>
       
        <p>
            References. 
            <ul>
                <li>There is back propagation exercise at NLPS, Week 1. </li>
            </ul>
        </p>
        <p>We explain by example. Suppose we want to classify an image. 
            To input: we turn this image into an array or a matrix. These are equivalent. 
            <ul><li>We want to retain the <i>spatial aspect</i>.</li></ul>
        </p>
        <p>
            Three aspects: 
            <ul>
                <li>Sparse connectivity.</li>
                <li>  Shared weights, runtime is the same
                    but storage is reduced. </li>
            </ul>
        </p>
    </section>


</html>

