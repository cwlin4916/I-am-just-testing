<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>Machine Learning</title>
  </head>
  <body>

    <section> Problem of data.
      <ul>
        <li> Limitation of <i>language</i>.
          This something we gain from going to neural networks.
        </li>
        <li>Neural networks abandoned 2010-11, 2012 was the transition point.
          The origin of the boost come from the Image Net challenge.
          There was superhuman performance beyond 2014. </li>
          <li>Have to be careful on how to apply Neural networks - small scale large scale,
            for instance  <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613"> here.
           </a>   Or the invisibility shirt  <a href="https://www.cs.umd.edu/~tomg/projects/invisible/">here</a>.
          </li>
          <li>Why is there redundancy in text? It reduces noise. </li>
      </ul>
    </section>


<section>
  Encoding language.
  <ul>
    <li> The message that carries most "information" are those with least redundancy. </li>
<li> Information is given by Shanon: h(x)=log 1/p(x), p(x) is probability.
  i.e. compressing a file, the limit to which you can compress is the limit.  </li>
  <li>Entropy quantifies the amount of suprise. </li>
  <li> Shanon information of letters.  </li>
  <li>The Ludwig Boltzan formula for entropy is a special case. </li>
  </ul>
</section>


<p>Understanding is about compressing. "Physics can be thought of as a compression."
  A good "theory" reduces entropy of our understanding.
</p>

<p>Entroy in maps. The angles, colors, spacing, this reduces the entropy.
Interestingly, subway map of newyork has been rotated. </p>
  </body>

  <p>Maximum entorpy distribution.
    <li>The table. </li>
  </p>
  <p>Distribution of people income.</p>
  <p> distribution of Galaxy sizes.</p>
</html>
