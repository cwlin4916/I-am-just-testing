<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>Machine Learning</title>
  </head>
  <footer>
   These are personal scramble (incomprehensible) of dinnertable talk ideas. 
   Here are further links to go into detail of certain topics:
   <ul>
    <li><a href="../subpages/applications_ml.html">applications</a>, particularly towards SDGs.</li>
    <li> <a href="../subpages/neural_network.html">Neural networks</a>. </li>
   </ul> 
  </footer>
  <body>

<section>
  References I find useful: 
  <ul>
    <li>Blog post by <a href="https://e2eml.school/blog.html">Brandon Rohrer</a>.  </li>
  </ul>
</section>

<section>
  <h3>
    Machine interpretability
        </h3>

        <p>
         <a href="https://www.youtube.com/watch?v=_DYQdP_F-LA&list=TLPQMDUwMjIwMjOQxo_kUtGMLw&index=6">why model understanding</a>?
          <ul>
            <li>Debugginng  </li>
            <li> Model understanding helps detect biases/deciding whether a model should be used. Decision makers, such as doctors or judges. </li>
            <li>Recourse  </li>
            <li> Vetting from suitability. </li>
          </ul>
          So how does one acheive model understanding? 
          <ol>
            <li> Inherently explainable models, Trees. </li>
            <li> Explain pre-built modles in a post-hoc manner. </li>
          </ol>
          In fact there is a trade off. 
        </p>
</section>




    <h4>Thinking things in terms of entropy</h4>
<section>
  Encoding language.
  <ul>
    <li> The message that carries most "information" are those with least redundancy. </li>
<li> Information is given by Shanon: h(x)=log 1/p(x), p(x) is probability.
  i.e. compressing a file, the limit to which you can compress is the limit.  </li>
  <li>Entropy quantifies the amount of suprise, The Ludwig Boltzan formula for entropy is a special case. </li>
  <li> Shanon information of letters.  </li>
  </ul>
</section>


<p>Understanding is about compressing. "Physics can be thought of as a compression."
  A good "theory" reduces entropy of our understanding.
</p>

<p>Entroy in maps. The angles, colors, spacing, this reduces the entropy.
Interestingly, subway map of newyork has been rotated. </p>
  </body>

  <p>Maximum entorpy distribution.
    <li>The table. </li>
  </p>
  <p>Distribution of people income; distribution of Galaxy sizes.</p>
<section>

</section>
<section>
  <h4>   Classification techniques
    </h4>
    <p> This  is in order of how I learnt them. 
    <ul>
      <li><li>logistic regression </li>
      <li>tSNE;  UMAP.</li>
      
    </ul>  </p> 
    <p> softmax, sparsemax, sparsegen. From proof of argument, how much do I again from understanding KKT? </p>
</section>

<section>
 <h4>Networks.</h4> 
  <p>Sparse; Degree distribution, the 6 degree, Milgram's experiment, and the small wold graph;  Graph traversal. </p>
  <p> Minimal path - this is NP hard; Clustering coefficient. </p>
</section>


<section>
  <h4>Symbolic and subsymbolic AI</h4>
  <p>Symbolic, GOFAI.  
    <ul>
      <li>Much easier for tasks where rules are well-defiend. </li>
      <li>1969,  Minsky and Papert, said subsymbolic is sterile.</li>
      <li>1970s, MYCIN. </li>
    </ul>
  </p>
  <p>Subsymbolic.
    <ul>
      <li>
        Frank Rosenblatt, 1950s, invented perceptron.  
      </li>
      <li>
        Explainable AI, research in IBM. 
      </li>
    </ul>
  </p>
</section>



<section>
  <h4>Problem of overfittinng.</h4>
  <ul>
    <li>
      Feature selection. 
    </li>
    <li>Regularization.</li>
  </ul>
</section>



<section> 
  <h4>Random stuffs.</h4>
  <ul>
    <li>Kevin Kelly, "â€œThe business plans of the next 10,000 startups are easy to forecast: Take X and add AI". Here AI is most likely deep learning. </li>
    </li>
    <li>Neural networks : one  boost come from the Image Net challenge.
      There was superhuman performance beyond 2014. </li>
      <li>Have to be careful on how to apply Neural networks - small scale large scale,
        for instance  <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613"> here.
       </a>   Or the invisibility shirt  <a href="https://www.cs.umd.edu/~tomg/projects/invisible/">here</a>.
      </li>
      <li>Why is there redundancy in text? It reduces noise. </li>
  </ul>
</section>


<p> Back to  <a href="#top">top</a> or <a href="../home.html">main page</a>. </p>



</html>
