<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>Machine Learning</title>
  </head>

  <body>


<h5>Useful references </h5>
    <p>
      <ul>
        <li>Blog post by <a href="https://e2eml.school/blog.html">Brandon Rohrer</a>.  </li>
      </ul>
    </p>
    
      <p>
       These are personal scramble (incomprehensible) of dinnertable talk ideas. 
       Here are further links to go into detail of certain topics:
       <ul>
        <li><a href="../subpages/applications_ml.html">applications</a>, particularly towards SDGs.</li>
        <li> <a href="../subpages/neural_network.html">neural networks</a>. </li>
        <li> <a href="../subpages/machine_interprebility.html">machine interpretability</a>. </li>
       </ul> 
      </p>

      <h5>Stupid questions (i.e. google would have answer)</h5>
      <ul>
        <li>What's the difference between colab and jupyter notebook. </li>
      </ul>

    <h4>Thinking things in terms of entropy</h4>
<section>
  Encoding language.
  <ul>
    <li> The message that carries most "information" are those with least redundancy. </li>
<li> Information is given by Shanon: h(x)=log 1/p(x), p(x) is probability.
  i.e. compressing a file, the limit to which you can compress is the limit.  </li>
  <li>Entropy quantifies the amount of suprise, The Ludwig Boltzan formula for entropy is a special case. </li>
  <li> Shanon information of letters.  </li>
  </ul>
</section>


<p>Understanding is about compressing. "Physics can be thought of as a compression."
  A good "theory" reduces entropy of our understanding.
</p>

<p>Entroy in maps. The angles, colors, spacing, this reduces the entropy.
Interestingly, subway map of newyork has been rotated. </p>
  </body>

  <p>Maximum entorpy distribution.
    <li>The table. </li>
  </p>
  <p>Distribution of people income; distribution of Galaxy sizes.</p>
<section>

</section>
<section>
  <h4>   Classification techniques
    </h4>
    <p> This  is in order of how I learnt them. 
    <ul>
      <li><li>logistic regression </li>
      <li>tSNE;  UMAP.</li>
      
    </ul>  </p> 
    <p> softmax, sparsemax, sparsegen. From proof of argument, how much do I again from understanding KKT? </p>
</section>

<section>
 <h4>Networks.</h4> 
  <p>Sparse; Degree distribution, the 6 degree, Milgram's experiment, and the small wold graph;  Graph traversal. </p>
  <p> Minimal path - this is NP hard; Clustering coefficient. </p>
</section>


<section>
  <h4>Symbolic and subsymbolic AI</h4>
  <p>Symbolic, GOFAI.  
    <ul>
      <li>Much easier for tasks where rules are well-defiend. </li>
      <li>1969,  Minsky and Papert, said subsymbolic is sterile.</li>
      <li>1970s, MYCIN. </li>
    </ul>
  </p>
  <p>Subsymbolic.
    <ul>
      <li>
        Frank Rosenblatt, 1950s, invented perceptron.  
      </li>
      <li>
        Explainable AI, research in IBM. 
      </li>
    </ul>
  </p>
</section>



<section>
  <h4>Problem of overfittinng.</h4>
  <ul>
    <li>
      Feature selection. 
    </li>
    <li>Regularization.</li>
  </ul>
</section>



<section> 
  <h4>
    Thigs to check oput
  </h4>
  <ul>
    <li>
      How <a href="https://jonathan-hui.medium.com/alphago-how-it-works-technically-26ddcc085319">alpha go works</a>. 
    </li>
  </ul>
  <h4>Random stuffs.</h4>
  <ul>
    <li>Kevin Kelly, "â€œThe business plans of the next 10,000 startups are easy to forecast: Take X and add AI". Here AI is most likely deep learning. </li>
  </li> 
    
  </ul>
</section>


<p> Back to  <a href="#top">top</a> or <a href="../home.html">main page</a>. </p>



</html>
