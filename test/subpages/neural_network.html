<!DOCTYPE html>
<html>


<section>
    References.
    <ul><li>
       <a href="../../Computer Science/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning-MIT (2017).pdf">Deep Learning</a>,  MIT. 
    </li></ul>
</section>
<section>
    <h3>What are questions that ML try to address?</h3>

    <p>
        <ul>
            <li>Generalization - the ablity to perform well on new inputs.
                This implies, to even improve performance we need  a <em>test set</em> and <em>train set</em>.
                Now one has a balance
                <ul>
                    <li>training  error small,  i.e. <em>underfittig</em>.</li>
                    <li>Disparity between training and test error small. <em>overfitting</em>. </li>
                </ul>
               See  Fig 5.3.  One measure of possibility of overfitting is that of <em>capacity</em>. 
            </li>
            
        </ul>

    </p>
</section>


    <section>
    <h4>Neural networks</h4> 
    <p>
    Some practical problems
    <ul>
    <li>
      How many hidden layers/units should I use?
    </li>
    <li>What makes neural network better than the perceptron?</li>
    </ul>
    
    </p>
    </section>


    <section>
    <p>The classic text processing. 
        <ul>

        </ul>
    </p>
    </section>

    <section>

        <h5>Is there a general way to pictorialize the provess of ML algorithms?</h5>


    </section>


    <section>
        <h3>Review of Supervised learning</h3>

        <p>
            In terms of probability, given x and y...? <br> 
            The flow is broken down as 
            <ol>
                <li>Input. </li>
                <li> Minimize Error. i.e. MSE. </li>
                <li>Apply gradient descent.</li>
            </ol>

            Ex. in the case of Linear regression, this is the  <em>normal equations</em>.  
        </p>

        <p>Regularization.</p>
    </section>
    <section>
        <h4>Review of feed forward networks</h4>
<p>  Why can one not approximate nonlinaer functions? <br>
Ex: take the XOR  function. </p>
    </section>



    <section>
        <h3>
            Convolution neural nets
        </h3>
        <p>
            Three aspects: 
            <ul>
                <li>Sparse connectivity.</li>
                <li>  Shared weights, runtime is the same
                    but storage is reduced. </li>
            </ul>
        </p>
    </section>


</html>

