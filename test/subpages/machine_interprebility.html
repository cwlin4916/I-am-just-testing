<!DOCTYPE html>
<html>

<section>
    <h3>
      Machine interpretability
          </h3>
  
          <p>In literature, there is 
            <i>interpretability</i>  and <i>explainability</i>. 
            <ul>
              <li> Inherently explainable models, Trees. </li>
              <li> Explain pre-built modles in a post-hoc manner. </li>
            </ul>
          Is there a trade off? 
            <ul><li>Cynthia Rudin <a href="https://www.youtube.com/watch?v=HY2NoQQzRic">argues</a> that 
              interprable machine learning is all we need.</li></ul>  AI may appear to know what they are doing. But there are some <a href="https://arxiv.org/abs/2302.03494">clear failures</a>.  
              Let us highlight somme instances.
            <ul><li>
              How CNNs are biased towards <a href="https://openreview.net/forum?id=Bygh9j09KX">texture</a>.  Similarly,  <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613"> global shape.
              </a> 
            </li>
          <li>   Or the invisibility shirt  <a href="https://www.cs.umd.edu/~tomg/projects/invisible/">here.</a>.
          </li>
          </ul>
            Further reasons for
                    <a href="https://www.youtube.com/watch?v=_DYQdP_F-LA&list=TLPQMDUwMjIwMjOQxo_kUtGMLw&index=6">why model understanding</a>
            <ul>
              <li>Debugginng  </li>
              <li> Model understanding helps detect biases/deciding whether a model should be used. Decision makers, such as doctors or judges. </li>
              <li>Recourse  </li>
              <li> Vetting from suitability. </li>
            </ul
          </p>
  </section>
  
  
</html>